{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the ICML 2018 Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of notebooks, we will replicate and analyze the ICML 2018 experiments that were used for benchmarking in <a href=\"http://proceedings.mlr.press/v80/falkner18a.html\" target=\"_blank\">BOHB (Falkner et al. 2018)</a>.\n",
    "In addition to <a href=\"https://github.com/automl/HpBandSter\" target=\"_blank\">HpBandSter</a>, we will use <a href=\"https://github.com/automl/CAVE\" target=\"_blank\">CAVE</a> to analyze and visualize the optimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this experiment\n",
    "\n",
    "### Cartpole\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\" (<a href=\"https://gym.openai.com/envs/CartPole-v0/\" target=\"_blank\">openAI gym description</a>)\n",
    "\n",
    "### Installation requirements\n",
    "\n",
    "To run the experiments, please install the <a href=\"https://github.com/automl/BOHBsCAVE/blob/master/examples/icml_2018_experiments/requirements.txt\" target=\"_blank\">requirements</a>, e.g. `pip install -r examples/icml_2018_experiments/requirements.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This examples ships with all the code necessary to reproduce the experiment. Because it takes a few days to generate the data, the results of the optimization are provided in `examples/icml_2018_experiments/opt_results/` If you want to generate the data (from examples/icml_2018_experiments), just run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Running experiment (args: Namespace(dataset_bnn=None, dataset_paramnet_surrogates='mnist', dest_dir='opt_results/cartpole/bohb', eta=3, exp_name='cartpole', max_budget=3.0, min_budget=1.0, n_workers=1, nic_name='lo', num_iterations=1, opt_method='bohb', run_id=0, surrogate_path=None, worker=False, working_directory='/tmp/'))\n",
      "1\n",
      "127.0.0.1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "10:37:13 WORKER: Connected to nameserver <Pyro4.core.Proxy at 0x7f1a341309b0; connected IPv4; for PYRO:Pyro.NameServer@127.0.0.1:37643>\n",
      "Getting optimizer...\n",
      "10:37:13 WORKER: No dispatcher found. Waiting for one to initiate contact.\n",
      "10:37:13 WORKER: start listening for jobs\n",
      "Initialization successful, starting optimization\n",
      "10:37:13 wait_for_workers trying to get the condition\n",
      "10:37:13 DISPATCHER: started the 'discover_worker' thread\n",
      "10:37:13 DISPATCHER: started the 'job_runner' thread\n",
      "10:37:13 DISPATCHER: Pyro daemon running on 127.0.0.1:42073\n",
      "10:37:13 DISPATCHER: Starting worker discovery\n",
      "10:37:13 DISPATCHER: Found 1 potential workers, 0 currently in the pool.\n",
      "10:37:13 DISPATCHER: discovered new worker, hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:37:13 HBMASTER: number of workers changed to 1\n",
      "10:37:13 Enough workers to start this run!\n",
      "10:37:13 adjust_queue_size: lock accquired\n",
      "10:37:13 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "10:37:13 HBMASTER: starting run at 1557823033.6704566\n",
      "10:37:13 HBMASTER: adjusted queue size to (0, 1)\n",
      "10:37:13 DISPATCHER: Finished worker discovery\n",
      "10:37:13 start sampling a new configuration.\n",
      "10:37:13 DISPATCHER: Trying to submit another job.\n",
      "10:37:13 done sampling a new configuration.\n",
      "10:37:13 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "10:37:13 HBMASTER: schedule new run for iteration 0\n",
      "10:37:13 HBMASTER: trying submitting job (0, 0, 0) to dispatcher\n",
      "10:37:13 HBMASTER: submitting job (0, 0, 0) to dispatcher\n",
      "10:37:13 DISPATCHER: trying to submit job (0, 0, 0)\n",
      "10:37:13 DISPATCHER: trying to notify the job_runner thread.\n",
      "10:37:13 HBMASTER: job (0, 0, 0) submitted to dispatcher\n",
      "10:37:13 DISPATCHER: Trying to submit another job.\n",
      "10:37:13 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "10:37:13 DISPATCHER: starting job (0, 0, 0) on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:37:13 DISPATCHER: job (0, 0, 0) dispatched on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:37:13 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "10:37:13 WORKER: start processing job (0, 0, 0)\n",
      "10:37:13 WORKER: args: ()\n",
      "10:37:13 WORKER: kwargs: {'config': {'batch_size': 27, 'discount': 0.1502835272627151, 'entropy_regularization': 0.053283813604174, 'learning_rate': 0.04195119904482327, 'likelihood_ratio_clipping': 0.6021681349897998, 'n_units_1': 12, 'n_units_2': 10}, 'budget': 1.0, 'working_directory': 'opt_results/cartpole/bohb'}\n",
      "WARNING:tensorflow:From /home/shuki/Repos/BOHBsCAVE/.ve_BOHBsCAVE/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "10:37:14 From /home/shuki/Repos/BOHBsCAVE/.ve_BOHBsCAVE/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/shuki/Repos/BOHBsCAVE/.ve_BOHBsCAVE/lib/python3.6/site-packages/tensorforce/models/model.py:1081: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10:37:14 From /home/shuki/Repos/BOHBsCAVE/.ve_BOHBsCAVE/lib/python3.6/site-packages/tensorforce/models/model.py:1081: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/shuki/Repos/BOHBsCAVE/.ve_BOHBsCAVE/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "10:37:15 From /home/shuki/Repos/BOHBsCAVE/.ve_BOHBsCAVE/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "10:37:19 Graph was finalized.\n",
      "2019-05-14 10:37:19.370104: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-05-14 10:37:19.658290: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494215000 Hz\n",
      "2019-05-14 10:37:19.681277: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x7f19d42781d0 executing computations on platform Host. Devices:\n",
      "2019-05-14 10:37:19.681320: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "10:37:20 Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "10:37:20 Done running local_init_op.\n",
      "10:37:52 WORKER: done with job (0, 0, 0), trying to register it.\n",
      "10:37:52 WORKER: registered result for job (0, 0, 0) with dispatcher\n",
      "10:37:52 DISPATCHER: job (0, 0, 0) finished\n",
      "10:37:52 DISPATCHER: register_result: lock acquired\n",
      "10:37:52 DISPATCHER: job (0, 0, 0) on hpbandster.run_0.worker.deepthought.31025139751667803968 finished\n",
      "10:37:52 job_id: (0, 0, 0)\n",
      "kwargs: {'config': {'batch_size': 27, 'discount': 0.1502835272627151, 'entropy_regularization': 0.053283813604174, 'learning_rate': 0.04195119904482327, 'likelihood_ratio_clipping': 0.6021681349897998, 'n_units_1': 12, 'n_units_2': 10}, 'budget': 1.0, 'working_directory': 'opt_results/cartpole/bohb'}\n",
      "result: {'loss': 3000.0, 'info': {'function_value': 3000.0, 'cost': 38.483118534088135, 'all_runs': [3000]}}\n",
      "exception: None\n",
      "\n",
      "10:37:52 job_callback for (0, 0, 0) started\n",
      "10:37:52 DISPATCHER: Trying to submit another job.\n",
      "10:37:52 job_callback for (0, 0, 0) got condition\n",
      "10:37:52 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "10:37:52 Only 1 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "10:37:52 HBMASTER: Trying to run another job!\n",
      "10:37:52 job_callback for (0, 0, 0) finished\n",
      "10:37:52 start sampling a new configuration.\n",
      "10:37:52 done sampling a new configuration.\n",
      "10:37:52 HBMASTER: schedule new run for iteration 0\n",
      "10:37:52 HBMASTER: trying submitting job (0, 0, 1) to dispatcher\n",
      "10:37:52 HBMASTER: submitting job (0, 0, 1) to dispatcher\n",
      "10:37:52 DISPATCHER: trying to submit job (0, 0, 1)\n",
      "10:37:52 DISPATCHER: trying to notify the job_runner thread.\n",
      "10:37:52 HBMASTER: job (0, 0, 1) submitted to dispatcher\n",
      "10:37:52 DISPATCHER: Trying to submit another job.\n",
      "10:37:52 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "10:37:52 DISPATCHER: starting job (0, 0, 1) on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:37:52 DISPATCHER: job (0, 0, 1) dispatched on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:37:52 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "10:37:52 WORKER: start processing job (0, 0, 1)\n",
      "10:37:52 WORKER: args: ()\n",
      "10:37:52 WORKER: kwargs: {'config': {'batch_size': 15, 'discount': 0.7065680021481467, 'entropy_regularization': 0.7032958440092998, 'learning_rate': 1.1993729087992069e-07, 'likelihood_ratio_clipping': 0.28562514083046386, 'n_units_1': 65, 'n_units_2': 128}, 'budget': 1.0, 'working_directory': 'opt_results/cartpole/bohb'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "10:37:56 Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "10:37:57 Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "10:37:57 Done running local_init_op.\n",
      "10:38:56 WORKER: done with job (0, 0, 1), trying to register it.\n",
      "10:38:56 WORKER: registered result for job (0, 0, 1) with dispatcher\n",
      "10:38:56 DISPATCHER: job (0, 0, 1) finished\n",
      "10:38:56 DISPATCHER: register_result: lock acquired\n",
      "10:38:56 DISPATCHER: job (0, 0, 1) on hpbandster.run_0.worker.deepthought.31025139751667803968 finished\n",
      "10:38:56 job_id: (0, 0, 1)\n",
      "kwargs: {'config': {'batch_size': 15, 'discount': 0.7065680021481467, 'entropy_regularization': 0.7032958440092998, 'learning_rate': 1.1993729087992069e-07, 'likelihood_ratio_clipping': 0.28562514083046386, 'n_units_1': 65, 'n_units_2': 128}, 'budget': 1.0, 'working_directory': 'opt_results/cartpole/bohb'}\n",
      "result: {'loss': 3000.0, 'info': {'function_value': 3000.0, 'cost': 64.56317663192749, 'all_runs': [3000]}}\n",
      "exception: None\n",
      "\n",
      "10:38:56 job_callback for (0, 0, 1) started\n",
      "10:38:56 job_callback for (0, 0, 1) got condition\n",
      "10:38:56 DISPATCHER: Trying to submit another job.\n",
      "10:38:56 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "10:38:56 Only 2 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "10:38:56 HBMASTER: Trying to run another job!\n",
      "10:38:56 job_callback for (0, 0, 1) finished\n",
      "10:38:56 start sampling a new configuration.\n",
      "10:38:56 done sampling a new configuration.\n",
      "10:38:56 HBMASTER: schedule new run for iteration 0\n",
      "10:38:56 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "10:38:56 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "10:38:56 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "10:38:56 DISPATCHER: trying to notify the job_runner thread.\n",
      "10:38:56 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "10:38:56 DISPATCHER: Trying to submit another job.\n",
      "10:38:56 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "10:38:56 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:38:56 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:38:56 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "10:38:56 WORKER: start processing job (0, 0, 2)\n",
      "10:38:56 WORKER: args: ()\n",
      "10:38:56 WORKER: kwargs: {'config': {'batch_size': 79, 'discount': 0.9850823674883354, 'entropy_regularization': 0.053872862146221, 'learning_rate': 0.0032786460995769305, 'likelihood_ratio_clipping': 0.5097695221337541, 'n_units_1': 112, 'n_units_2': 20}, 'budget': 1.0, 'working_directory': 'opt_results/cartpole/bohb'}\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "10:39:02 Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "10:39:02 Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "10:39:02 Done running local_init_op.\n",
      "10:45:02 WORKER: done with job (0, 0, 2), trying to register it.\n",
      "10:45:02 WORKER: registered result for job (0, 0, 2) with dispatcher\n",
      "10:45:02 DISPATCHER: job (0, 0, 2) finished\n",
      "10:45:02 DISPATCHER: register_result: lock acquired\n",
      "10:45:02 DISPATCHER: job (0, 0, 2) on hpbandster.run_0.worker.deepthought.31025139751667803968 finished\n",
      "10:45:02 job_id: (0, 0, 2)\n",
      "kwargs: {'config': {'batch_size': 79, 'discount': 0.9850823674883354, 'entropy_regularization': 0.053872862146221, 'learning_rate': 0.0032786460995769305, 'likelihood_ratio_clipping': 0.5097695221337541, 'n_units_1': 112, 'n_units_2': 20}, 'budget': 1.0, 'working_directory': 'opt_results/cartpole/bohb'}\n",
      "result: {'loss': 2449.0, 'info': {'function_value': 2449.0, 'cost': 365.7932026386261, 'all_runs': [2449]}}\n",
      "exception: None\n",
      "\n",
      "10:45:02 job_callback for (0, 0, 2) started\n",
      "10:45:02 DISPATCHER: Trying to submit another job.\n",
      "10:45:02 job_callback for (0, 0, 2) got condition\n",
      "10:45:02 DISPATCHER: jobs to submit = 0, number of idle workers = 1 -> waiting!\n",
      "10:45:02 Only 3 run(s) for budget 1.000000 available, need more than 9 -> can't build model!\n",
      "10:45:02 HBMASTER: Trying to run another job!\n",
      "10:45:02 job_callback for (0, 0, 2) finished\n",
      "10:45:02 ITERATION: Advancing config (0, 0, 2) to next budget 3.000000\n",
      "10:45:02 HBMASTER: schedule new run for iteration 0\n",
      "10:45:02 HBMASTER: trying submitting job (0, 0, 2) to dispatcher\n",
      "10:45:02 HBMASTER: submitting job (0, 0, 2) to dispatcher\n",
      "10:45:02 DISPATCHER: trying to submit job (0, 0, 2)\n",
      "10:45:02 DISPATCHER: trying to notify the job_runner thread.\n",
      "10:45:02 HBMASTER: job (0, 0, 2) submitted to dispatcher\n",
      "10:45:02 HBMASTER: running jobs: 1, queue sizes: (0, 1) -> wait\n",
      "10:45:02 DISPATCHER: Trying to submit another job.\n",
      "10:45:02 DISPATCHER: starting job (0, 0, 2) on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:45:02 DISPATCHER: job (0, 0, 2) dispatched on hpbandster.run_0.worker.deepthought.31025139751667803968\n",
      "10:45:02 DISPATCHER: jobs to submit = 0, number of idle workers = 0 -> waiting!\n",
      "10:45:02 WORKER: start processing job (0, 0, 2)\n",
      "10:45:02 WORKER: args: ()\n",
      "10:45:02 WORKER: kwargs: {'config': {'batch_size': 79, 'discount': 0.9850823674883354, 'entropy_regularization': 0.053872862146221, 'learning_rate': 0.0032786460995769305, 'likelihood_ratio_clipping': 0.5097695221337541, 'n_units_1': 112, 'n_units_2': 20}, 'budget': 3.0, 'working_directory': 'opt_results/cartpole/bohb'}\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "10:45:07 Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "10:45:07 Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "10:45:07 Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "! python run_experiment.py --exp_name cartpole --num_iterations 1 --min_budget 1 --max_budget 3 --n_workers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "You can use this script to generate the optimization data for a variety of optimizers, the results will (by default) be stored in `examples/icml_2018_experiments/EXPERIMENT[/DATASET]/OPTIMIZER`. Use `python run_experiment --help` to see how to use the script.\n",
    "If you have access to a cluster, take a look at the scripts provided for cluster computation on a SLURM cluster at `examples/icml_2018_experiments/scripts/cluster/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the results in CAVE\n",
    "\n",
    "### Instantiate CAVE\n",
    "\n",
    "Analyzing the optimization results with CAVE is very straight-forward. If you want to use CAVE interactively in a notebook, set `show_jupyter=True`. Specify which optimization you want to analyze via the `folders` argument and specify `file_format==SMAC3` or `file_format==BOHB`, depending on which optimizer was used for the results. To analyze how BOHB optimized the cartpole-problem, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from cave.cavefacade import CAVE\n",
    "\n",
    "cave = CAVE(folders=[\"opt_results/cartpole/\"],\n",
    "            output_dir=\"CAVE_reports/cartpole_notebook\",  # output for debug/images/etc\n",
    "            ta_exec_dir=[\".\"],                            # Only important for SMAC-results\n",
    "            file_format='BOHB',                           # BOHB or SMAC3\n",
    "            verbose_level='OFF',\n",
    "            show_jupyter=True,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the HTML-report you can use the `analyze`-method. The report is located in `output_dir/report.html`, so in this case in `CAVE_reports/cartpole_notebook/report.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cave.analyze()\n",
    "! firefox CAVE_reports/cartpole_notebook/report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAVE is fully compatible with Jupyter notebooks. You can invoke the individual analysis methods as follows.\n",
    "\n",
    "The most interesting plot for BOHB might be a visualization of the learning curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.bohb_learning_curves();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.overview_table();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each budget, we can list the cost over incumbents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.bohb_incumbents_per_budget();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For parameter-importance analysis, CAVE uses <a href=\"https://github.com/automl/ParameterImportance\" target=\"_blank\">PIMP</a> , a package that provides multiple approaches to parameter-importance analysis. We can easily invoke them via CAVE, of course. To estimate the importance, random forests are used to predict performances of configurations that were not executed. This is difficult for big budgets with few configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the individual budgets via the 'run'-keyword-argument of each analysis-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.cave_fanova(run='budget_10000');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.local_parameter_importance(run='budget_10000');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each budget, we can compare the different parameter-importance-methods that have already been run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cave.pimp_comparison_table(run='budget_10000');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze BOHB's behaviour, we can check out the configurator footprint, cost-over-time and parallel coordinated parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.configurator_footprint(use_timeslider=True, num_quantiles=5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.cost_over_time();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cave.parallel_coordinates(run='budget_10000');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BOHBsCAVE",
   "language": "python",
   "name": "bohbscave"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
